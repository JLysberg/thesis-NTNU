\section{Dataset} \label{sec:impl/dataset}

Raw data from the eye tracker was output in 16 channels. 
% Since the sampling rate was constant at 1200Hz, some samples were missing due to eye occlusion. Technical defects also caused single data points to be lost at regular intervals. 
When data from one source was unavailable due to pupil occlusion or technical issues, the corresponding channel was filled with NaN-values. The raw data channels are presented in table \ref{tab:impl/dataset/raw}. This table also gives a short description of each channel and the rate of missing data.

\import{.}{datatable_raw}

\subsection{Pre-Processing}

Raw data was not immediately suitable as a training dataset. Since many channels contained a high rate of missing values, these had to either be interpolated or dropped. Then, blinks events and \acrshort{ebr} could be extrapolated and added as a new channel.

\subsubsection{Data cleaning}

The first step of the data cleaning process was to remove all samples recorded before the session began. These samples were easily distinguished by their lack of a "Presented Stimulus name." Then, a new data channel was filled with the average pupil diameter between the left and right eye. Finally, insignificant one- and two-sample data losses were interpolated with the last valid data point.

\subsubsection{Blink Extrapolation}

Blinks were detected by applying algorithm \ref{alg:impl/blink_extrapolation}. Its parameters were tuned by trial and error while observing the dataset's internal characteristics. Lines 2-6 looks for long-duration "EyesNotFound" eye movements and stores their indices in memory. Then, lines 8-21 further extend these blink intervals if short-duration "Unclassified" eye movements come within ten consecutive samples. In the end, lines 23-27 remove blinks from the raw input data and create a new blink rate channel.

\begin{algorithm}
    \caption{Blink extrapolation algorithm. EMT and GED is short for "Eye Movement Types" and "Gaze Event Durations", and represent two channels from the raw data with the same name. Note that the iterative representation of this algoritm is just for ease of understanding. In practice, it was implemented with efficient inplace data processing libraries.}
    \label{alg:impl/blink_extrapolation}
    \begin{algorithmic}[1]
        \Procedure{EXTRAPOLATE}{$data$} %\Comment{The IDT of vector, with thresholds}
            \For{$sample$ \textbf{in} $data$}
                \If{$sample.EMT="EyesNotFound"$ \textbf{and} $sample.GED>5$}
                    \State $BlinkIndices \gets data.index$
                \EndIf
            \EndFor
            \State
            \For{$i \gets 1$ \textbf{to} $3$}
                \For{$sample$ \textbf{in} $data$}
                    \If{$sample.index - 10$ \textbf{in} $BlinkIndices$}
                        \If{$sample.EMT="Unclassified"$ \textbf{and} $sample.GED<2$}
                            \State $BlinkIndices \gets data.index$
                        \EndIf
                    \EndIf
                \EndFor
                \For{$index$ \textbf{in} $BlinkIndices$}
                    \If{num. samples since last $index > 10$}
                        \State $BlinkIndices \gets$ all indices since last $index$
                    \EndIf
                \EndFor
            \EndFor
            \State
            \State $data \gets data - (data$ \textbf{where} $data.index = BlinkIndices)$
            \State
            \For{$sample$ \textbf{in} $data$}
                \State $BlinkRate \gets $ num. $BlinkIndices$ in a window surrounding $sample$
            \EndFor
            \State
            \State \textbf{return} $BlinkRate$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\subsubsection{Smoothing and Normalizing}

A low-pass filter was applied to all data channels after the blink rate channel had been added. Then, the pupillary- and blink rate channels were scaled by minimum and maximum values to normalized levels between zero and one.

\subsection{Labeling} \label{sec:impl/dataset/labeling}

% After pre-processing, a final step was needed to create a fully-fledged dataset fit for training. 
After pre-processing, the data could finally be used for dataset creation. To that end, the author chose four sets of labels, each generating one dataset intended to capture one inherent correlation within the data.

\subsubsection{Levels}

The first two datasets were labeled by the predefined difficulty levels of the N-Back task, as described in section \ref{sec:impl/tasks}. By the assumptions made in section \ref{sec:impl/data_acq}, the labels of this dataset are intended to capture levels of cognitive load imposed on the subject. Since each difficulty occurs the same number of times throughout the data, the labeling scheme ensures a fully balanced dataset. Furthermore, greater correlations may artificially be induced if the dataset is split into only levels 0 and 2. Throughout the following discussions, these datasets will be referred to as \acrfull{fl} and \acrfull{bl}.

\subsubsection{States}

In addition to the level datasets, we will employ a labeling scheme that distinguishes between the transient states of the N-back task during execution. As was briefly mentioned in section \ref{sec:impl/tasks}, figure \ref{fig:impl/NBackBlockSeg} has labeled each N-Back block segment with "idle", "onset", "execution", and "offset". 
% These are intended to capture correlations that appear in the data from phasic task-evoked responses. 
As discussed in section \ref{sec:bt/cognitive_impacts}, some cognitive states may be detected by task-evoked pupillary and \acrshort{ebr} responses. This labeling scheme is intended to capture these correlations. Again, we add another dataset with fewer labels to induce larger correlations, with only the "idle" and "execution" states. We call these datasets \acrfull{fs} and \acrfull{bs}.
% We hope to capture these correlations by introducing this labeling scheme. 

\subsection{Segmentation, Subsampling, and Augmentation}

Finally, the processed and labeled datasets are molded for Pytorch's tensor-based deep learning framework by arranging samples in fixed-length segments. Since data was recorded at frequencies beyond what was needed, these segments could be created by subsampling every fourth sample of the original data. This way, the dataset could essentially be augmented from one hour of 1200Hz data to four hours of 300Hz data. Segments were chosen to be 256 samples in length, which could represent about 50s in real-time. Each sample was then assigned a \textit{segment ID}. The target labels for each segment are added to a separate dataset.

The final dataset includes four data channels, as shown in figure \ref{tab:impl/dataset/processed}. They are pupil diameter, blink rate (\acrshort{ebr}), and on-screen gaze in the X- and Y- dimensions. A fifth channel indicates which segment each sample belongs to, which determines its target label for classification. 
% The data channel dataset is shown on the left in table \ref{tab:impl/dataset/processed}, and target labels are on the right.

\import{.}{datatable_processed}

% \textit{long-segment subsampled and augmented turned out to provide the overall best results. Was hence chosen as a constant for all models and training procedures.}

% \subsection{Task Exposure Effects}

% TODO: Add plot of data in blocks representing experience and fatigue
