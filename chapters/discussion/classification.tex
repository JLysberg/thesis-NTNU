\section{Classification Models}

From tables \ref{tab:res/performance} and \ref{tab:res/training}, there is no denying that the architecture of a classification model matters. On the one hand, simpler models require less computing resources and time to train but are far surpassed by the more complex models in terms of classification accuracy. In the end, the confusion matrices of section \ref{sec:res/confusion} are what ultimately indicate how the classification models may be compared in a production setting. 
% It is also worth noting that the number of trainable parameters is not the only thing that determines the complexity of training. In fact, the \acrshort{mlp}, although having the second-most number of parameters, was the fastest model to train. 
% Additionally, as we can see from the training history plots of section \ref{sec:res/training}, each model has a different tendency to overfit the training data.

% \textit{Performance of ResNet in this approach compared to others may be due to limitations in dataset, as deeper models require much more data in order to generalize well [Fawaz].}


Overall, it is clear that the \acrshort{cnn}s are a winner for \acrlong{tsc}. This result is similar to what \textcite{fawaz2018} found in their review. Contrary to their results, however, \acrshort{fcn} outperforms both \acrshort{resnet11} and \acrshort{resnet18} for two out of four datasets. These findings are surprising, considering that deeper networks are known to perform better than shallower networks for computer vision \cite{he2015}. One reasonable justification would blame limitations in the dataset for this aberration. While deep models often produce better results than their shallow counterparts, they require much larger datasets to generalize well. This is natural, given that higher complexity raises the risk of overfitting the training data.
In any case, while this outperformance was only marginal by one or two percentage points, \acrshort{resnet18} dominated \acrshort{fcn} by almost 12 percent when trained on the \acrshort{bl} dataset. 
% In fact, it seems that the deeper residual networks may have an inherent advantage in distinguishing between the clearer distinctions between classes which is present in the binary case.
% In fact, state-of-the-art image classification models have grown from just seven layers in 2012 to over a thousand layers in 2016. The consensus seems to be that the deeper the model is, the more nuances can be captured within the data. Even still, one can only make such conclusions with immensely large datasets.

% The unbalanced nature of the labels in the state label groups made the classification results from the two fully connected models very uninteresting, and their results were hence excluded from table \ref{tab:res/performance}.These models showed no tendency to learn from the data presented, and however long they were trained, they did not modify their weights beyond making constant predictions of one class. As mentioned in section \ref{sec:impl/clf_models}, the fully connected nature of these models ensured that all temporal invariance in the input data was lost. As such, only individual activations of every single neuron in the input were weighted in making predictions. For datasets such as \acrshort{fsds} and \acrshort{bsds}, the temporal pattern in the input makes such a significance in class destinction that the fully connected architectures had no chance at making accurate predictions. Therefore, what was observed when training these models on a particularly temporally variant dataset, which was additionally heavily unbalanced, was that the models could obtain better classification results by simply predicting the class which was most abundant, while fully ignoring all other classes. blablabla skrib om plis.

The lack of temporal invariance in the \acrshort{mlp} and \acrshort{mcdcnn} architectures, as highlighted in section \ref{sec:impl/clf_models}, was made clear when they were trained on the \acrshort{fs} and \acrshort{bs} datasets. Contrary to \acrshort{fl} and \acrshort{bl}, classes of the state label groups have more pronounced temporal distinctions in their input channels. For instance, the onset state shows a positive slope pattern in the pupillary channel, and a negative slope in the blink rate channel. Since the convolutional nature of \acrshort{fcn}, \acrshort{resnet11}, and \acrshort{resnet18} allow for the creation of feature maps that encapsulate patterns in time, they are naturally suited to perform well on such datasets. \acrshort{mlp} and \acrshort{mcdcnn}, however, were unable to improve their weights beyond making chance predictions. It is for this reason that some cells are excluded from table \ref{tab:res/performance}.

As expected, the deeper convolutional networks took much longer to train than \acrshort{mlp} and \acrshort{mcdcnn}. In fact, even though \acrshort{fcn} had less than half the amount of parameters as \acrshort{mlp}, it took over four times as long to train. Of course, this is caused by the way in which trainable parameters are scattered throughout each architecture. Instead of learning individual weights for every input to every output node in every layer, convolutional networks learn internal weights of shared filters, each representing one feature in the input. However, training is still more computationally expensive since these filters must be evaluated across the entire time dimension.

% The state label groups relies more on the distinction between signal shape in time.
% For a dataset with where classes depend on the general shape of a input segment

% When applying the \acrshort{mlp} and \acrshort{mcdcnn} architectures on the state label groups, 
% Classification accuracies from predictions on the state label groups by the \acrshort{mlp} and \acrshort{mcdcnn} architectures were excluded from this table. 

\subsection{Confusion Matrices} \label{sec:disc/classification/cm}

% Looking at figures \ref{fig:res/conf_fstate} and \ref{fig:res/conf_bstate}, one can see a clear concentration of classifications along the diagonal cells, with some exceptions. On the \acrshort{fs} dataset, the idle and onset labels seem particularly difficult to distinguish from execution and onset, likely because these classes have very distinct features which are captured by the convolutional filters. Especially for the execution state, the sawtooth pattern caused by consistent blink periods would explain its high rate of correct predictions. The phenomenon is even more pronounced in the \acrshort{bs} dataset, where the only other class is the idle state. 

% Another interesting observation is the large number of clean 0s in the idle column of figure \ref{fig:res/conf_bstate}. This would mean that the only 

Although accuracy is a good metric to evaluate a model's overall generalized performance for every training step, the confusion matrices of section \ref{sec:res/confusion} allow us to look closer at its ability to predict individual classes. All matrices have been normalized along their rows, such that row cells illustrate probability distributions that sum to one. Doing so aligns values such that the cells on the diagonal represent per-class recall
% , as discussed in section \ref{sec:bt/DNN/terms}
. The same cells would instead represent per-class precision had values been normalized along columns. However, since we are interested in discussing the properties of true classes in the dataset, the recall metric was deemed reasonable for this thesis.

First, looking at the classifications of the state label groups in figure \ref{fig:res/conf_state}, we can see a clear pattern of correct predictions along the matrix diagonals. However, some segments are more distinctly recognizable in the data than others. The execution and offset classes are correctly recalled in over 80\% of cases, while idle is mostly misclassified. Onset, too, is often misclassified as being offset. The characteristic sawtooth pattern of the execution class explains how its classification performs so well. By looking at the gaze channels of figure \ref{fig:res/statelevel_xy}, one may explain the discrepancy with the idle class by comparing some of the transients that it exhibits compared to the onset and offset classes. It is hard to recognize patterns that positively place a segment in the idle class even by human judgment. Combined with the fact that the state dataset is highly imbalanced (few examples of idle segments are presented to the model during training), poor performance in this class is to be expected.
% Additionally, the state dataset is highly imbalanced, such that very few examples of idle segments are presented to the model during training. 

By removing the onset and offset classes entirely, as in the \acrshort{bs} dataset, we get the confusion matrix of figure \ref{fig:res/conf_bstate}. Now, 33\% of the previously misclassified idle segments add up to a recall of 64\%. Although an improvement, it is not at all perfect. Again, the imbalanced nature of the state dataset may be to blame. Recall of the execution class is also greatly improved, up to an impressive 100\%.

The results from level classification, presented in figure \ref{fig:res/conf_level}, show a similarly distinct pattern on the matrix diagonal. However, since classes of the level label groups are both fully balanced (each level is represented in 33\% of all segments) and less temporally invariant, we see more of a spread in cell values. Interestingly, recall is almost 15\% higher for levels 1 and 2 than for level 0. This result is reassuring for the case of cognitive load classification. It signifies that there is indeed a data pattern in these more cognitively demanding levels that the model picks up. In particular, the lack of consistency of data from level 0 compared to 1 and 2, mentioned in section \ref{sec:disc/dataset}, may explain why level 0 is more often misclassified. 


% Discuss model classification Accuracies and confusion matrices

% Discuss model complexity, training time etc 